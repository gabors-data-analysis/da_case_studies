name: Test Notebooks

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:  # Allows manual triggering

jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # Extended timeout for notebook execution + data download

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Data and Paths
        run: |
          # 1. Download and set up data
          # Go to workspace root (parent of repo)
          cd ..
          WORKSPACE_ROOT=$(pwd)
          echo "Workspace root: $WORKSPACE_ROOT"
          
          # Ensure da_data_repo exists at the level expected by notebooks
          # Notebooks expect: dirname + "da_data_repo"
          # dirname = .../work/ from split logic
          # So we need .../work/da_data_repo
          # Current dir is .../work/da_case_studies
          # So we need ../da_data_repo
          
          # BUT split logic is fragile.
          # Path: /home/runner/work/da_case_studies/da_case_studies
          # split("da_case_studies") -> /home/runner/work/
          
          # So notebooks look for: /home/runner/work/da_data_repo
          # This is ../../da_data_repo from here
          
          TARGET_DATA_DIR="../../da_data_repo"
          
          echo "Downloading datasets..."
          curl -L "https://files.osf.io/v1/resources/3u5em/providers/osfstorage/?zip=" -o data.zip
          unzip -q data.zip
          rm data.zip
          
          if [ -f "da_data_repo.zip" ]; then
             unzip -q da_data_repo.zip
             rm da_data_repo.zip
          fi
          
          # Handle extracted folder scenarios
          if [ -d "da_data_repo" ]; then
             # Flatten nested da_data_repo if it exists
             if [ -d "da_data_repo/da_data_repo" ]; then
                rsync -a da_data_repo/da_data_repo/ da_data_repo/
                rm -rf da_data_repo/da_data_repo
             fi
             mv da_data_repo "$TARGET_DATA_DIR"
          else
             # Assuming loose files or single wrapper
             dir_count=$(find . -maxdepth 1 -mindepth 1 -type d | wc -l)
             if [ "$dir_count" -eq 1 ]; then
                single_dir=$(find . -maxdepth 1 -mindepth 1 -type d)
                mv "$single_dir" "$TARGET_DATA_DIR"
             else
                # Move everything else
                mkdir -p "$TARGET_DATA_DIR"
                mv * "$TARGET_DATA_DIR/" 2>/dev/null || true
             fi
          fi
          
          # 2. Fix 'data_out' paths using Symlinks
          # Notebooks write to: dirname + "da_case_studies/chXX..."
          # This resolves to: /home/runner/work/da_case_studies/chXX...
          # But the repo is at: /home/runner/work/da_case_studies/da_case_studies/chXX...
          # So we need to symlink chXX folders into /home/runner/work/da_case_studies/
          
          echo "Symlinking chapter folders..."
          # We are in .../work/da_case_studies/da_case_studies/.. (i.e. .../work/da_case_studies)
          # Link children of da_case_studies (the repo) to here
          
          ln -s da_case_studies/ch* .
          ln -s da_case_studies/da* .
          ln -s da_case_studies/pre* . || true
          
          echo "Symlink verification:"
          ls -la ch04-management-firm-size || echo "Symlink failed"

      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y graphviz

      - name: Set up Miniconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniforge-variant: Miniforge3
          miniforge-version: latest
          activate-environment: daenv
          environment-file: ch00-tech-prep/daenv_linux.yml
          python-version: "3.12"
          auto-activate-base: false

      - name: Display environment info
        shell: bash -el {0}
        run: |
          conda info
          python --version

      - name: Install CmdStan (required for Prophet)
        shell: bash -el {0}
        run: |
          python -c "import cmdstanpy; cmdstanpy.install_cmdstan()"

      - name: Run notebook tests
        shell: bash -el {0}
        env:
          MPLBACKEND: Agg
        run: |
          python ch00-tech-prep/test_env.py

      - name: Upload test artifacts on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: failed-test-outputs
          path: |
            **/*.py
            !ch00-tech-prep/test_env.py
          retention-days: 7
