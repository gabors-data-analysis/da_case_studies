name: Test Notebooks

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:  # Allows manual triggering

jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # Extended timeout for notebook execution + data download

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Clean Workspace
        id: setup_workspace
        run: |
          # Create a clean workspace structure to avoid path duplication issues
          # Notebooks rely on split("da_case_studies") which fails when path is .../da_case_studies/da_case_studies
          
          # New structure will be:
          # /home/runner/work/workspace/
          # ├── da_case_studies/ (The repo)
          # └── da_data_repo/    (The data)
          
          WORKSPACE_ROOT="/home/runner/work/custom_workspace"
          mkdir -p "$WORKSPACE_ROOT"
          echo "WORKSPACE_ROOT=$WORKSPACE_ROOT" >> $GITHUB_ENV
          
          # Move repo to new location
          mv $GITHUB_WORKSPACE "$WORKSPACE_ROOT/da_case_studies"
          
          echo "Workspace setup at $WORKSPACE_ROOT"

      - name: Download and Setup Data
        run: |
          cd "$WORKSPACE_ROOT"
          
          # Target data directory
          TARGET_DATA_DIR="$WORKSPACE_ROOT/da_data_repo"
          
          # Download
          echo "Downloading datasets..."
          curl -L "https://files.osf.io/v1/resources/3u5em/providers/osfstorage/?zip=" -o data.zip
          
          # Extract
          unzip -q data.zip
          rm data.zip
          
          # Handle nested zip if present
          if [ -f "da_data_repo.zip" ]; then
             unzip -q da_data_repo.zip
             rm da_data_repo.zip
          fi
          
          # Normalize folder structure
          # If extracted to a single folder, move content to target
          dir_count=$(find . -maxdepth 1 -mindepth 1 -type d -not -name "da_case_studies" | wc -l)
          if [ "$dir_count" -eq 1 ]; then
             extracted_dir=$(find . -maxdepth 1 -mindepth 1 -type d -not -name "da_case_studies")
             if [ "$extracted_dir" != "./da_data_repo" ]; then
               mv "$extracted_dir" da_data_repo
             fi
          fi
          
          # Verify structure
          echo "Structure of $WORKSPACE_ROOT:"
          ls -la "$WORKSPACE_ROOT"
          
          if [ -d "da_data_repo/da_data_repo" ]; then
             echo "Flattening nested da_data_repo..."
             rsync -a da_data_repo/da_data_repo/ da_data_repo/
             rm -rf da_data_repo/da_data_repo
          fi

      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y graphviz

      - name: Set up Miniconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniforge-variant: Miniforge3
          miniforge-version: latest
          activate-environment: daenv
          environment-file: ${{ env.WORKSPACE_ROOT }}/da_case_studies/ch00-tech-prep/daenv_linux.yml
          python-version: "3.12"
          auto-activate-base: false

      - name: Display environment info
        shell: bash -el {0}
        run: |
          conda info
          python --version

      - name: Install CmdStan (required for Prophet)
        shell: bash -el {0}
        run: |
          python -c "import cmdstanpy; cmdstanpy.install_cmdstan()"

      - name: Run notebook tests
        shell: bash -el {0}
        env:
          MPLBACKEND: Agg
        working-directory: ${{ env.WORKSPACE_ROOT }}/da_case_studies
        run: |
          python ch00-tech-prep/test_env.py

      - name: Upload test artifacts on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: failed-test-outputs
          path: |
            ${{ env.WORKSPACE_ROOT }}/da_case_studies/**/*.py
            !${{ env.WORKSPACE_ROOT }}/da_case_studies/ch00-tech-prep/test_env.py
          retention-days: 7
